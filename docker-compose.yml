version: '3.8'

services:
  resume-insight-pro:
    build: .
    ports:
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - WEB3FORMS_ACCESS_KEY=${WEB3FORMS_ACCESS_KEY}
      - API_URL=http://ollama:11434/api/generate  # Updated API URL
    volumes:
      - .:/app  # Mount the project directory to the container
    depends_on:
      - ollama  # Ensure Ollama service starts first
    container_name: resume-insight-pro-container
    deploy:
      resources:
        limits:
          cpus: '2'  # Adjust the CPU allocation for better control in Cloud Run
          memory: 4G  # Limit memory to a reasonable value for Cloud Run

  ollama:
    image: ollama/ollama:latest  # Ollama service image
    ports:
      - "11434:11434"  # Expose API port for Ollama
    container_name: ollama-container
    volumes:
      - ~/.ollama/models:/root/.ollama/models  # Mount the models directory from the host to the container
    deploy:
      resources:
        limits:
          memory: 8G  # Set memory limit for the Ollama service to match Cloud Run capabilities
        reservations:
          memory: 4G  # Reserved memory for Ollama service
    environment:
      - OLLAMA_MAX_MEMORY=8G  # Adjusted to 8GB for Google Cloud Run's capabilities
      - BATCH_SIZE=128  # Lower the batch size to manage resource load on Cloud Run
      - CONTEXT_SIZE=2048  # Further reduce context size to lower memory footprint
